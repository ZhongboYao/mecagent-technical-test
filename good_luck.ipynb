{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv (or do it you're own way)\n",
    "2. Run `uv sync`\n",
    "3. Run `source .venv/bin/activate`\n",
    "\n",
    "You're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The Task : Create the best CadQuery code generator model. \n",
    "\n",
    "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
    "2. Create a baseline model and evaluate it with the given metrics.\n",
    "3. Enhance by any manner the baseline model and evaluate it again.\n",
    "4. Explain you choices and possible bottlenecks. \n",
    "5. Show what enhancements you would have done if you had more time.\n",
    "\n",
    "You can do *WHATEVER* you want, be creative, result is not what matters the most. \n",
    "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
    "\n",
    "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
    "2. Best IOU assess the similarity between the meshes generated by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage of the metrics\n",
    "sample_code = \"\"\"\n",
    "height = 60.0\n",
    "width = 80.0\n",
    "thickness = 10.0\n",
    "diameter = 22.0\n",
    "\n",
    "# make the base\n",
    "result = (\n",
    "    cq.Workplane(\"XY\")\n",
    "    .box(height, width, thickness)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sample_code_2 = \"\"\"\n",
    " height = 60.0\n",
    " width = 80.0\n",
    " thickness = 10.0\n",
    " diameter = 22.0\n",
    " padding = 12.0\n",
    "\n",
    " # make the base\n",
    " result = (\n",
    "     cq.Workplane(\"XY\")\n",
    "     .box(height, width, thickness)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .hole(diameter)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .rect(height - padding, width - padding, forConstruction=True)\n",
    "     .vertices()\n",
    "     .cboreHole(2.4, 4.4, 2.1)\n",
    " )\n",
    "\"\"\"\n",
    "\n",
    "codes = {\n",
    "    \"sample_code\": sample_code,\n",
    "    \"sample_code_2\": sample_code_2,\n",
    "}\n",
    "vsr = evaluate_syntax_rate_simple(codes)\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "iou = get_iou_best(sample_code, sample_code_2)\n",
    "print(\"IOU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "I just lost my access to my university HPC, therefore all codes are run on Mac Air with M1 chip.<br>\n",
    "Even running a full validation takes 9hrs.<br>\n",
    "Therefore, all the below codes are only tested to run successfully but the results cannot be obtained.<br>\n",
    "I also tried to run on GoogleColab later but only !uv sync takes a lot of time, leaving insufficient time for the later execution.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best\n",
    "from pprint import pprint\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.nn import GELU\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\", \"validation\"])\n",
    "\n",
    "train= ds[0]\n",
    "test = ds[1]\n",
    "val = ds[2]\n",
    "\n",
    "example = train[0]\n",
    "pprint(example)\n",
    "\n",
    "print(\"Prompt:\", example['prompt'])\n",
    "print(\"CADQuery Code:\\n\", example['cadquery'])\n",
    "print(\"ID:\", example['deepcad_id'])\n",
    "\n",
    "example['image'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Inspired by [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/html/2505.14646v1?utm_source=chatgpt.com) but switched models to lighter ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Encoder (CLIP ViT-B/32), frozen\n",
    "vision_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "# Decoder (CodeGen-350M)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\n",
    "decoder = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-multi\").to(device)\n",
    "\n",
    "# MLP Projector (fine-tuned with rest)\n",
    "class MLPProjector(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=decoder.config.n_embd):  # 512â†’1024 for CodeGen\n",
    "        super().__init__()\n",
    "        self.proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, output_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "projector = MLPProjector().to(device)\n",
    "\n",
    "vision_model.eval()\n",
    "projector.eval()\n",
    "decoder.eval()\n",
    "\n",
    "for m in [vision_model, projector, decoder]:\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "generated_codes = []  \n",
    "ground_truths = [] \n",
    "\n",
    "for example in tqdm(val, desc=\"Running inference\"):\n",
    "    image = example[\"image\"].convert(\"RGB\")\n",
    "    prompt = \"\"\"\n",
    "    Generate CadQuery code for this shape, using the following structure as the reference:\\n\n",
    "    height = 60.0\n",
    "    width = 80.0\n",
    "    thickness = 10.0\n",
    "    diameter = 22.0\n",
    "    padding = 12.0\n",
    "\n",
    "    # make the base\n",
    "    result = (\n",
    "        cq.Workplane(\"XY\")\n",
    "        .box(height, width, thickness)\n",
    "        .faces(\">Z\")\n",
    "        .workplane()\n",
    "        .hole(diameter)\n",
    "        .faces(\">Z\")\n",
    "        .workplane()\n",
    "        .rect(height - padding, width - padding, forConstruction=True)\n",
    "        .vertices()\n",
    "        .cboreHole(2.4, 4.4, 2.1)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vision_embed = vision_model.get_image_features(**inputs)        \n",
    "        projected_embed = projector(vision_embed).unsqueeze(1) \n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_embeds = decoder.get_input_embeddings()(input_ids)         \n",
    "    inputs_embeds = torch.cat([projected_embed, prompt_embeds], dim=1)\n",
    "\n",
    "    generated_ids = decoder.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    generated_codes.append(output_text)\n",
    "\n",
    "    if \"cadquery\" in example:\n",
    "        ground_truths.append(example[\"cadquery\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = {}\n",
    "total_IoU = 0\n",
    "for i, pred, truth in enumerate(zip(generated_codes, ground_truths)):\n",
    "    key = f'code[i]'\n",
    "    codes[key] = pred\n",
    "    total_IoU += get_iou_best(pred, truth)\n",
    "    vsr = evaluate_syntax_rate_simple(codes)\n",
    "\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "print(\"IOU:\", total_IoU/len(generated_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement1: Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def custom_collate(batch):\n",
    "    for example in batch:\n",
    "        example[\"image\"] = transform(example[\"image\"])\n",
    "    return default_collate(batch)  # now safe\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "projector = MLPProjector().to(device)\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    image = batch[\"image\"]\n",
    "    cadquery_code = batch[\"cadquery\"]\n",
    "    prompt = \"Generate CadQuery code for this shape:\\n\"\n",
    "\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.squeeze(0).to(device)\n",
    "    target_ids = tokenizer(cadquery_code, return_tensors=\"pt\").input_ids.squeeze(0).to(device)\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(device)\n",
    "    return inputs, prompt_ids, target_ids\n",
    "\n",
    "vision_model.eval()\n",
    "for param in vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "projector.train()\n",
    "decoder.train()\n",
    "\n",
    "optimizer = AdamW(list(projector.parameters()) + list(decoder.parameters()), lr=2e-5)\n",
    "\n",
    "for epoch in range(50):  \n",
    "    loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        inputs, prompt_ids, target_ids = preprocess_batch(batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_embed = vision_model.get_image_features(**inputs) \n",
    "\n",
    "        projected_embeds = projector(image_embed).unsqueeze(1)\n",
    "        prompt_embeds = decoder.get_input_embeddings()(prompt_ids.unsqueeze(0))\n",
    "        code_embeds = decoder.get_input_embeddings()(target_ids.unsqueeze(0))\n",
    "\n",
    "        inputs_embeds = torch.cat([projected_embeds, prompt_embeds, code_embeds], dim=1) \n",
    "\n",
    "        prefix_mask = torch.full((1, 11), -100, dtype=torch.long, device=target_ids.device)\n",
    "        labels = torch.cat([prefix_mask, target_ids.unsqueeze(0)], dim=1).to(device)\n",
    "\n",
    "        outputs = decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loader.set_postfix(loss=f\"{loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Ideas\n",
    "1. Augment the images by changing the view angle and use the views from multiple angles to enhance the result. Both late fusion and early fusion can be tried.\n",
    "2. In reality where we don't have true generation codes, multiple views can be synthesized using NeRF-family neural networks.\n",
    "3. Estimate the depth map of the component using depth estimation models, then build a point cloud from it. Rotate the component to align with x, y, z axis and the extract key features like height, width, length to inform the decoder. It can be done by feeding the information to the prompt, or be embedded into feature tokens.\n",
    "4. Attach a second agent (like ChatGPT) to correct grammar mistakes by the code generator.\n",
    "5. Randomly mask some parts of the image (not key parts like a hole), then train the model as usual. This can be generalized to normal image augmentation, like rotation, change the brightness, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecagent-technical-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
